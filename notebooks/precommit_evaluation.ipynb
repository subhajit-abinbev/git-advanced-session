{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614f5195",
   "metadata": {},
   "source": [
    "# Pre-Commit Configuration Evaluation\n",
    "\n",
    "This notebook provides a comprehensive evaluation of the `.pre-commit-config.yaml` file, focusing on:\n",
    "- ğŸ“Š **File Size Checks** - Preventing large file commits\n",
    "- ğŸ” **Secret Detection** - Multiple layers of security scanning\n",
    "- ğŸ““ **Notebook Stripouts** - Jupyter notebook cleaning\n",
    "- âš™ï¸ **Configuration Analysis** - Best practices and recommendations\n",
    "\n",
    "## Evaluation Criteria\n",
    "- âœ… **Security**: Does it prevent sensitive data leaks?\n",
    "- âœ… **Performance**: Are the hooks efficient and fast?\n",
    "- âœ… **Maintainability**: Is the configuration easy to maintain?\n",
    "- âœ… **Completeness**: Does it cover all necessary checks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429df73a",
   "metadata": {},
   "source": [
    "## 1. Load and Parse the YAML Configuration\n",
    "\n",
    "First, let's load the pre-commit configuration file and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c8512",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.11.13)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Load the pre-commit configuration\n",
    "config_path = Path(\"../.pre-commit-config.yaml\")\n",
    "\n",
    "try:\n",
    "    with open(config_path, 'r', encoding='utf-8') as file:\n",
    "        precommit_config = yaml.safe_load(file)\n",
    "    \n",
    "    print(\"âœ… Successfully loaded .pre-commit-config.yaml\")\n",
    "    print(f\"ğŸ“„ Configuration has {len(precommit_config.get('repos', []))} repository hooks\")\n",
    "    \n",
    "    # Display basic structure\n",
    "    print(\"\\nğŸ“‹ Configuration Overview:\")\n",
    "    for i, repo in enumerate(precommit_config.get('repos', []), 1):\n",
    "        repo_name = repo['repo'].split('/')[-1] if '/' in repo['repo'] else repo['repo']\n",
    "        hook_count = len(repo.get('hooks', []))\n",
    "        print(f\"  {i}. {repo_name} ({hook_count} hooks)\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ .pre-commit-config.yaml not found!\")\n",
    "except yaml.YAMLError as e:\n",
    "    print(f\"âŒ YAML parsing error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading configuration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf090d",
   "metadata": {},
   "source": [
    "## 2. Analyze File Size Check Configuration\n",
    "\n",
    "Let's examine the `check-added-large-files` hook to evaluate its effectiveness in preventing large file commits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc93973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and analyze file size check configuration\n",
    "file_size_analysis = {\n",
    "    \"found\": False,\n",
    "    \"max_size_kb\": None,\n",
    "    \"max_size_mb\": None,\n",
    "    \"recommendation\": \"âŒ Not configured\"\n",
    "}\n",
    "\n",
    "for repo in precommit_config.get('repos', []):\n",
    "    for hook in repo.get('hooks', []):\n",
    "        if hook.get('id') == 'check-added-large-files':\n",
    "            file_size_analysis[\"found\"] = True\n",
    "            \n",
    "            # Extract maxkb argument\n",
    "            args = hook.get('args', [])\n",
    "            for arg in args:\n",
    "                if '--maxkb=' in arg:\n",
    "                    kb_value = int(arg.split('=')[1])\n",
    "                    file_size_analysis[\"max_size_kb\"] = kb_value\n",
    "                    file_size_analysis[\"max_size_mb\"] = round(kb_value / 1024, 2)\n",
    "                    break\n",
    "\n",
    "print(\"ğŸ“Š File Size Check Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if file_size_analysis[\"found\"]:\n",
    "    print(\"âœ… check-added-large-files hook is configured\")\n",
    "    \n",
    "    if file_size_analysis[\"max_size_kb\"]:\n",
    "        size_kb = file_size_analysis[\"max_size_kb\"]\n",
    "        size_mb = file_size_analysis[\"max_size_mb\"]\n",
    "        print(f\"ğŸ“ Maximum file size: {size_kb} KB ({size_mb} MB)\")\n",
    "        \n",
    "        # Evaluate the size limit\n",
    "        if size_kb <= 500:  # 500KB\n",
    "            print(\"ğŸŸ¢ Excellent: Very strict size limit prevents almost all large files\")\n",
    "            file_size_analysis[\"recommendation\"] = \"ğŸŸ¢ Excellent configuration\"\n",
    "        elif size_kb <= 1000:  # 1MB\n",
    "            print(\"ğŸŸ¡ Good: Reasonable size limit for most projects\")\n",
    "            file_size_analysis[\"recommendation\"] = \"ğŸŸ¡ Good configuration\"\n",
    "        elif size_kb <= 5000:  # 5MB\n",
    "            print(\"ğŸŸ  Fair: Size limit may allow some large files\")\n",
    "            file_size_analysis[\"recommendation\"] = \"ğŸŸ  Consider reducing limit\"\n",
    "        else:\n",
    "            print(\"ğŸ”´ Poor: Size limit is too high\")\n",
    "            file_size_analysis[\"recommendation\"] = \"ğŸ”´ Reduce size limit\"\n",
    "    else:\n",
    "        print(\"âš ï¸  Default size limit (no --maxkb specified)\")\n",
    "        file_size_analysis[\"recommendation\"] = \"âš ï¸  Add explicit --maxkb argument\"\n",
    "else:\n",
    "    print(\"âŒ check-added-large-files hook is NOT configured\")\n",
    "    print(\"ğŸš¨ This is a critical security gap!\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Recommendation: {file_size_analysis['recommendation']}\")\n",
    "\n",
    "# Test with current repository files\n",
    "print(f\"\\nğŸ” Current Repository File Analysis:\")\n",
    "large_files = []\n",
    "for root, dirs, files in os.walk(\"..\"):\n",
    "    # Skip .git directory\n",
    "    if '.git' in root:\n",
    "        continue\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        try:\n",
    "            size_bytes = os.path.getsize(file_path)\n",
    "            size_kb = size_bytes / 1024\n",
    "            \n",
    "            # Check against configured limit or default 1MB\n",
    "            limit_kb = file_size_analysis.get(\"max_size_kb\", 1000)\n",
    "            \n",
    "            if size_kb > limit_kb:\n",
    "                large_files.append({\n",
    "                    \"path\": os.path.relpath(file_path, \"..\"),\n",
    "                    \"size_kb\": round(size_kb, 2),\n",
    "                    \"size_mb\": round(size_kb / 1024, 2)\n",
    "                })\n",
    "        except (OSError, IOError):\n",
    "            continue\n",
    "\n",
    "if large_files:\n",
    "    print(f\"âš ï¸  Found {len(large_files)} files exceeding size limit:\")\n",
    "    for file_info in large_files[:5]:  # Show first 5\n",
    "        print(f\"  ğŸ“„ {file_info['path']}: {file_info['size_kb']} KB ({file_info['size_mb']} MB)\")\n",
    "    if len(large_files) > 5:\n",
    "        print(f\"  ... and {len(large_files) - 5} more files\")\n",
    "else:\n",
    "    print(\"âœ… No files exceed the configured size limit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be5227",
   "metadata": {},
   "source": [
    "## 3. Evaluate Secret Detection Setup\n",
    "\n",
    "Analyzing the multi-layered secret detection configuration including detect-secrets and GitGuardian shield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8dbf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze secret detection configuration\n",
    "secret_detection = {\n",
    "    \"detect_private_key\": False,\n",
    "    \"detect_aws_credentials\": False,\n",
    "    \"detect_secrets\": False,\n",
    "    \"ggshield\": False,\n",
    "    \"baseline_file\": None,\n",
    "    \"exclusions\": [],\n",
    "    \"coverage_score\": 0\n",
    "}\n",
    "\n",
    "print(\"ğŸ” Secret Detection Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for repo in precommit_config.get('repos', []):\n",
    "    for hook in repo.get('hooks', []):\n",
    "        hook_id = hook.get('id')\n",
    "        \n",
    "        # Check for basic secret detection hooks\n",
    "        if hook_id == 'detect-private-key':\n",
    "            secret_detection[\"detect_private_key\"] = True\n",
    "            print(\"âœ… detect-private-key: Detects SSH/TLS private keys\")\n",
    "            \n",
    "        elif hook_id == 'detect-aws-credentials':\n",
    "            secret_detection[\"detect_aws_credentials\"] = True\n",
    "            print(\"âœ… detect-aws-credentials: Detects AWS credentials\")\n",
    "            \n",
    "        elif hook_id == 'detect-secrets':\n",
    "            secret_detection[\"detect_secrets\"] = True\n",
    "            print(\"âœ… detect-secrets: Advanced secret scanning\")\n",
    "            \n",
    "            # Check for baseline file\n",
    "            args = hook.get('args', [])\n",
    "            for arg in args:\n",
    "                if '--baseline' in arg and len(args) > args.index(arg) + 1:\n",
    "                    baseline_file = args[args.index(arg) + 1]\n",
    "                    secret_detection[\"baseline_file\"] = baseline_file\n",
    "                    print(f\"   ğŸ“„ Baseline file: {baseline_file}\")\n",
    "                    \n",
    "                    # Check if baseline file exists\n",
    "                    baseline_path = Path(f\"../{baseline_file}\")\n",
    "                    if baseline_path.exists():\n",
    "                        print(\"   âœ… Baseline file exists\")\n",
    "                    else:\n",
    "                        print(\"   âŒ Baseline file missing!\")\n",
    "            \n",
    "            # Check exclusions\n",
    "            exclude = hook.get('exclude')\n",
    "            if exclude:\n",
    "                secret_detection[\"exclusions\"].append(exclude)\n",
    "                print(f\"   ğŸš« Exclusions: {exclude}\")\n",
    "                \n",
    "        elif hook_id == 'ggshield':\n",
    "            secret_detection[\"ggshield\"] = True\n",
    "            print(\"âœ… ggshield: GitGuardian security scanning\")\n",
    "\n",
    "# Calculate coverage score\n",
    "coverage_components = [\n",
    "    secret_detection[\"detect_private_key\"],\n",
    "    secret_detection[\"detect_aws_credentials\"], \n",
    "    secret_detection[\"detect_secrets\"],\n",
    "    secret_detection[\"ggshield\"]\n",
    "]\n",
    "secret_detection[\"coverage_score\"] = sum(coverage_components)\n",
    "\n",
    "print(f\"\\nğŸ“Š Secret Detection Coverage Score: {secret_detection['coverage_score']}/4\")\n",
    "\n",
    "if secret_detection[\"coverage_score\"] == 4:\n",
    "    print(\"ğŸŸ¢ Excellent: Comprehensive multi-layered secret detection\")\n",
    "elif secret_detection[\"coverage_score\"] == 3:\n",
    "    print(\"ğŸŸ¡ Good: Strong secret detection with minor gaps\")\n",
    "elif secret_detection[\"coverage_score\"] == 2:\n",
    "    print(\"ğŸŸ  Fair: Basic secret detection, consider adding more layers\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor: Insufficient secret detection coverage\")\n",
    "\n",
    "# Test secret detection with our .env file\n",
    "print(f\"\\nğŸ§ª Testing Secret Detection with .env file:\")\n",
    "env_file_path = Path(\"../.env\")\n",
    "if env_file_path.exists():\n",
    "    print(\"âœ… .env file exists (contains fake secrets for testing)\")\n",
    "    \n",
    "    # Check if .env would be caught by gitignore\n",
    "    gitignore_path = Path(\"../.gitignore\") \n",
    "    if gitignore_path.exists():\n",
    "        with open(gitignore_path, 'r') as f:\n",
    "            gitignore_content = f.read()\n",
    "            if '.env' in gitignore_content:\n",
    "                print(\"âœ… .env file is properly ignored by .gitignore\")\n",
    "            else:\n",
    "                print(\"âŒ .env file is NOT in .gitignore!\")\n",
    "    \n",
    "    # Sample some content to show what would be detected\n",
    "    with open(env_file_path, 'r') as f:\n",
    "        env_content = f.read()\n",
    "        \n",
    "    # Count potential secrets\n",
    "    api_key_patterns = [\n",
    "        r'API_KEY\\s*=\\s*[^\\s]+',\n",
    "        r'SECRET\\s*=\\s*[^\\s]+', \n",
    "        r'TOKEN\\s*=\\s*[^\\s]+',\n",
    "        r'PASSWORD\\s*=\\s*[^\\s]+'\n",
    "    ]\n",
    "    \n",
    "    total_secrets = 0\n",
    "    for pattern in api_key_patterns:\n",
    "        matches = re.findall(pattern, env_content, re.IGNORECASE)\n",
    "        total_secrets += len(matches)\n",
    "    \n",
    "    print(f\"ğŸ¯ Found {total_secrets} potential secrets in .env file\")\n",
    "    print(\"   These would be caught by secret detection hooks if not properly ignored\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ .env file not found for testing\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Recommendations:\")\n",
    "if not secret_detection[\"detect_secrets\"]:\n",
    "    print(\"   ğŸ”¸ Add detect-secrets for comprehensive scanning\")\n",
    "if not secret_detection[\"ggshield\"]:\n",
    "    print(\"   ğŸ”¸ Add ggshield for GitGuardian integration\")\n",
    "if secret_detection[\"baseline_file\"] and not Path(f\"../{secret_detection['baseline_file']}\").exists():\n",
    "    print(\"   ğŸ”¸ Create the missing baseline file\")\n",
    "if secret_detection[\"coverage_score\"] == 4:\n",
    "    print(\"   ğŸŸ¢ Secret detection configuration is excellent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f911663",
   "metadata": {},
   "source": [
    "## 4. Review Notebook Stripout Configuration\n",
    "\n",
    "Examining the nbstripout hook that cleans Jupyter notebooks before committing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb205dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze notebook stripout configuration\n",
    "notebook_config = {\n",
    "    \"nbstripout_found\": False,\n",
    "    \"repo_info\": None,\n",
    "    \"version\": None,\n",
    "    \"notebook_count\": 0,\n",
    "    \"notebooks_with_outputs\": []\n",
    "}\n",
    "\n",
    "print(\"ğŸ““ Notebook Stripout Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check for nbstripout configuration\n",
    "for repo in precommit_config.get('repos', []):\n",
    "    for hook in repo.get('hooks', []):\n",
    "        if hook.get('id') == 'nbstripout':\n",
    "            notebook_config[\"nbstripout_found\"] = True\n",
    "            notebook_config[\"repo_info\"] = repo['repo']\n",
    "            notebook_config[\"version\"] = repo.get('rev', 'Unknown')\n",
    "            print(\"âœ… nbstripout hook is configured\")\n",
    "            print(f\"   ğŸ“¦ Repository: {repo['repo']}\")\n",
    "            print(f\"   ğŸ·ï¸  Version: {repo.get('rev', 'Not specified')}\")\n",
    "            break\n",
    "\n",
    "if not notebook_config[\"nbstripout_found\"]:\n",
    "    print(\"âŒ nbstripout hook is NOT configured\")\n",
    "    print(\"   ğŸš¨ Jupyter notebooks may contain outputs and metadata!\")\n",
    "\n",
    "# Scan for notebook files in the project\n",
    "print(f\"\\nğŸ” Scanning for Jupyter notebooks:\")\n",
    "notebook_files = []\n",
    "for root, dirs, files in os.walk(\"..\"):\n",
    "    if '.git' in root:\n",
    "        continue\n",
    "    for file in files:\n",
    "        if file.endswith('.ipynb'):\n",
    "            notebook_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(notebook_path, \"..\")\n",
    "            notebook_files.append(rel_path)\n",
    "\n",
    "notebook_config[\"notebook_count\"] = len(notebook_files)\n",
    "\n",
    "if notebook_files:\n",
    "    print(f\"ğŸ“Š Found {len(notebook_files)} Jupyter notebook(s):\")\n",
    "    for nb_path in notebook_files:\n",
    "        print(f\"   ğŸ“„ {nb_path}\")\n",
    "        \n",
    "        # Check if notebook has outputs\n",
    "        try:\n",
    "            full_path = Path(f\"../{nb_path}\")\n",
    "            with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                nb_content = json.load(f)\n",
    "                \n",
    "            has_outputs = False\n",
    "            output_count = 0\n",
    "            \n",
    "            for cell in nb_content.get('cells', []):\n",
    "                if cell.get('outputs'):\n",
    "                    has_outputs = True\n",
    "                    output_count += len(cell['outputs'])\n",
    "                if cell.get('execution_count'):\n",
    "                    has_outputs = True\n",
    "                    \n",
    "            if has_outputs:\n",
    "                notebook_config[\"notebooks_with_outputs\"].append({\n",
    "                    \"path\": nb_path,\n",
    "                    \"output_count\": output_count\n",
    "                })\n",
    "                print(f\"      âš ï¸  Has {output_count} outputs/execution data\")\n",
    "            else:\n",
    "                print(f\"      âœ… Clean (no outputs)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ Error reading notebook: {e}\")\n",
    "else:\n",
    "    print(\"   â„¹ï¸  No Jupyter notebooks found\")\n",
    "\n",
    "# Evaluate configuration effectiveness\n",
    "print(f\"\\nğŸ“Š Notebook Configuration Assessment:\")\n",
    "\n",
    "if notebook_config[\"nbstripout_found\"]:\n",
    "    if len(notebook_config[\"notebooks_with_outputs\"]) == 0:\n",
    "        print(\"ğŸŸ¢ Excellent: nbstripout is configured and all notebooks are clean\")\n",
    "        recommendation = \"ğŸŸ¢ Perfect configuration\"\n",
    "    else:\n",
    "        print(f\"ğŸŸ¡ Good: nbstripout is configured but {len(notebook_config['notebooks_with_outputs'])} notebook(s) have outputs\")\n",
    "        print(\"   ğŸ’¡ Run nbstripout manually on existing notebooks to clean them\")\n",
    "        recommendation = \"ğŸŸ¡ Clean existing notebooks\"\n",
    "else:\n",
    "    if notebook_config[\"notebook_count\"] > 0:\n",
    "        print(\"ğŸ”´ Critical: Notebooks found but nbstripout not configured!\")\n",
    "        recommendation = \"ğŸ”´ Add nbstripout immediately\"\n",
    "    else:\n",
    "        print(\"ğŸŸ¢ Good: No notebooks found, nbstripout not needed currently\")\n",
    "        recommendation = \"ğŸŸ¢ Add nbstripout when notebooks are added\"\n",
    "\n",
    "print(f\"\\nğŸ’¡ Recommendation: {recommendation}\")\n",
    "\n",
    "# Show benefits of nbstripout\n",
    "print(f\"\\nğŸ¯ Benefits of nbstripout:\")\n",
    "print(\"   âœ… Removes sensitive output data\")\n",
    "print(\"   âœ… Reduces repository size\")\n",
    "print(\"   âœ… Cleaner diffs and merges\")\n",
    "print(\"   âœ… Prevents execution metadata conflicts\")\n",
    "print(\"   âœ… Improves collaboration\")\n",
    "\n",
    "if notebook_config[\"notebooks_with_outputs\"]:\n",
    "    total_outputs = sum(nb[\"output_count\"] for nb in notebook_config[\"notebooks_with_outputs\"])\n",
    "    print(f\"\\nâš ï¸  Current risk: {total_outputs} outputs across {len(notebook_config['notebooks_with_outputs'])} notebooks\")\n",
    "    print(\"   These could contain sensitive data or large binary content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c742155",
   "metadata": {},
   "source": [
    "## 5. Check Hook Dependencies and Versions\n",
    "\n",
    "Verifying that all hooks are using current versions and checking for potential compatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ff30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check hook versions and dependencies\n",
    "version_analysis = {\n",
    "    \"repositories\": [],\n",
    "    \"outdated_count\": 0,\n",
    "    \"security_issues\": [],\n",
    "    \"total_hooks\": 0\n",
    "}\n",
    "\n",
    "print(\"ğŸ” Hook Version Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Known latest versions (as of 2025) - in real scenario, this would be fetched from APIs\n",
    "known_latest = {\n",
    "    \"pre-commit-hooks\": \"v4.6.0\",\n",
    "    \"detect-secrets\": \"v1.4.0\", \n",
    "    \"nbstripout\": \"0.7.1\",\n",
    "    \"black\": \"24.4.2\",\n",
    "    \"flake8\": \"7.0.0\",\n",
    "    \"isort\": \"5.13.2\",\n",
    "    \"mirrors-eslint\": \"v9.5.0\",\n",
    "    \"mirrors-prettier\": \"v4.0.0-alpha.8\",\n",
    "    \"ggshield\": \"v1.25.0\"\n",
    "}\n",
    "\n",
    "for repo_config in precommit_config.get('repos', []):\n",
    "    repo_url = repo_config['repo']\n",
    "    repo_name = repo_url.split('/')[-1] if '/' in repo_url else repo_url\n",
    "    current_version = repo_config.get('rev', 'Not specified')\n",
    "    \n",
    "    # Check if version is specified\n",
    "    if current_version == 'Not specified':\n",
    "        status = \"âš ï¸  No version specified\"\n",
    "        recommendation = \"Specify explicit version\"\n",
    "    else:\n",
    "        latest_version = known_latest.get(repo_name, \"Unknown\")\n",
    "        if latest_version != \"Unknown\":\n",
    "            if current_version == latest_version:\n",
    "                status = \"âœ… Up to date\"\n",
    "                recommendation = \"Good\"\n",
    "            else:\n",
    "                status = \"ğŸŸ¡ Potentially outdated\"\n",
    "                recommendation = f\"Consider updating to {latest_version}\"\n",
    "                version_analysis[\"outdated_count\"] += 1\n",
    "        else:\n",
    "            status = \"â“ Version unknown\"\n",
    "            recommendation = \"Manual check required\"\n",
    "    \n",
    "    hook_count = len(repo_config.get('hooks', []))\n",
    "    version_analysis[\"total_hooks\"] += hook_count\n",
    "    \n",
    "    version_analysis[\"repositories\"].append({\n",
    "        \"name\": repo_name,\n",
    "        \"url\": repo_url,\n",
    "        \"current_version\": current_version,\n",
    "        \"status\": status,\n",
    "        \"recommendation\": recommendation,\n",
    "        \"hook_count\": hook_count\n",
    "    })\n",
    "    \n",
    "    print(f\"ğŸ“¦ {repo_name}\")\n",
    "    print(f\"   ğŸ·ï¸  Version: {current_version}\")\n",
    "    print(f\"   ğŸ“Š Status: {status}\")\n",
    "    print(f\"   ğŸ”§ Hooks: {hook_count}\")\n",
    "    if recommendation != \"Good\":\n",
    "        print(f\"   ğŸ’¡ {recommendation}\")\n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "print(f\"ğŸ“Š Version Summary:\")\n",
    "print(f\"   ğŸ“¦ Total repositories: {len(version_analysis['repositories'])}\")\n",
    "print(f\"   ğŸ”§ Total hooks: {version_analysis['total_hooks']}\")\n",
    "print(f\"   ğŸŸ¡ Potentially outdated: {version_analysis['outdated_count']}\")\n",
    "\n",
    "# Check for security considerations\n",
    "print(f\"\\nğŸ”’ Security Analysis:\")\n",
    "\n",
    "security_checks = [\n",
    "    {\n",
    "        \"name\": \"Version pinning\",\n",
    "        \"check\": all(repo[\"current_version\"] != \"Not specified\" for repo in version_analysis[\"repositories\"]),\n",
    "        \"importance\": \"High\",\n",
    "        \"description\": \"All repositories should have pinned versions\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Recent versions\", \n",
    "        \"check\": version_analysis[\"outdated_count\"] <= 1,\n",
    "        \"importance\": \"Medium\",\n",
    "        \"description\": \"Most hooks should be reasonably current\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Security-focused hooks\",\n",
    "        \"check\": any(\"detect\" in repo[\"name\"] or \"ggshield\" in repo[\"name\"] for repo in version_analysis[\"repositories\"]),\n",
    "        \"importance\": \"High\", \n",
    "        \"description\": \"Security scanning hooks should be present\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for check in security_checks:\n",
    "    status = \"âœ…\" if check[\"check\"] else \"âŒ\"\n",
    "    print(f\"   {status} {check['name']} ({check['importance']} priority)\")\n",
    "    print(f\"      {check['description']}\")\n",
    "\n",
    "# Performance considerations\n",
    "print(f\"\\nâš¡ Performance Considerations:\")\n",
    "heavy_hooks = ['ggshield', 'eslint', 'black', 'flake8']\n",
    "configured_heavy = [repo[\"name\"] for repo in version_analysis[\"repositories\"] if repo[\"name\"] in heavy_hooks]\n",
    "\n",
    "if configured_heavy:\n",
    "    print(f\"   ğŸŒ Heavy hooks detected: {', '.join(configured_heavy)}\")\n",
    "    print(\"   ğŸ’¡ Consider using 'stages: [manual]' for expensive hooks during development\")\n",
    "else:\n",
    "    print(\"   âš¡ No particularly heavy hooks detected\")\n",
    "\n",
    "# CI configuration check\n",
    "ci_config = precommit_config.get('ci', {})\n",
    "if ci_config:\n",
    "    print(f\"\\nğŸš€ CI Configuration:\")\n",
    "    print(f\"   ğŸ”„ Auto-update: {'âœ…' if ci_config.get('autoupdate_schedule') else 'âŒ'}\")\n",
    "    print(f\"   ğŸ”§ Auto-fix PRs: {'âœ…' if ci_config.get('autofix_prs') else 'âŒ'}\")\n",
    "    print(f\"   ğŸ“… Update schedule: {ci_config.get('autoupdate_schedule', 'Not set')}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ No CI configuration found\")\n",
    "    print(\"   ğŸ’¡ Consider adding CI configuration for automated updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5d4b5",
   "metadata": {},
   "source": [
    "## 6. Validate YAML Structure and Syntax\n",
    "\n",
    "Performing structural validation and checking for proper pre-commit configuration syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate YAML structure and syntax\n",
    "validation_results = {\n",
    "    \"yaml_valid\": False,\n",
    "    \"structure_valid\": False,\n",
    "    \"required_fields\": [],\n",
    "    \"optional_fields\": [],\n",
    "    \"issues\": [],\n",
    "    \"score\": 0\n",
    "}\n",
    "\n",
    "print(\"ğŸ”§ YAML Structure Validation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. YAML Syntax Validation\n",
    "try:\n",
    "    # Re-load to ensure it's still valid\n",
    "    with open(config_path, 'r', encoding='utf-8') as file:\n",
    "        config_reloaded = yaml.safe_load(file)\n",
    "    validation_results[\"yaml_valid\"] = True\n",
    "    print(\"âœ… YAML syntax is valid\")\n",
    "except yaml.YAMLError as e:\n",
    "    validation_results[\"issues\"].append(f\"YAML syntax error: {e}\")\n",
    "    print(f\"âŒ YAML syntax error: {e}\")\n",
    "except Exception as e:\n",
    "    validation_results[\"issues\"].append(f\"File error: {e}\")\n",
    "    print(f\"âŒ File error: {e}\")\n",
    "\n",
    "# 2. Pre-commit Structure Validation\n",
    "if validation_results[\"yaml_valid\"]:\n",
    "    print(f\"\\nğŸ“‹ Structure Validation:\")\n",
    "    \n",
    "    # Check required top-level fields\n",
    "    required_top_level = ['repos']\n",
    "    optional_top_level = ['ci', 'default_language_version', 'default_stages', 'files', 'exclude']\n",
    "    \n",
    "    for field in required_top_level:\n",
    "        if field in precommit_config:\n",
    "            validation_results[\"required_fields\"].append(field)\n",
    "            print(f\"   âœ… Required field '{field}' present\")\n",
    "        else:\n",
    "            validation_results[\"issues\"].append(f\"Missing required field: {field}\")\n",
    "            print(f\"   âŒ Missing required field: {field}\")\n",
    "    \n",
    "    for field in optional_top_level:\n",
    "        if field in precommit_config:\n",
    "            validation_results[\"optional_fields\"].append(field)\n",
    "            print(f\"   â„¹ï¸  Optional field '{field}' present\")\n",
    "    \n",
    "    # 3. Repository Structure Validation\n",
    "    print(f\"\\nğŸ“¦ Repository Configuration Validation:\")\n",
    "    \n",
    "    if 'repos' in precommit_config and isinstance(precommit_config['repos'], list):\n",
    "        for i, repo in enumerate(precommit_config['repos']):\n",
    "            print(f\"\\n   Repository {i+1}:\")\n",
    "            \n",
    "            # Check required repo fields\n",
    "            required_repo_fields = ['repo', 'rev', 'hooks']\n",
    "            for field in required_repo_fields:\n",
    "                if field in repo:\n",
    "                    print(f\"      âœ… {field}: {repo[field] if field != 'hooks' else f'{len(repo[field])} hooks'}\")\n",
    "                else:\n",
    "                    issue = f\"Repository {i+1} missing required field: {field}\"\n",
    "                    validation_results[\"issues\"].append(issue)\n",
    "                    print(f\"      âŒ Missing {field}\")\n",
    "            \n",
    "            # Validate hooks structure\n",
    "            if 'hooks' in repo and isinstance(repo['hooks'], list):\n",
    "                for j, hook in enumerate(repo['hooks']):\n",
    "                    if 'id' not in hook:\n",
    "                        issue = f\"Repository {i+1}, Hook {j+1} missing required 'id' field\"\n",
    "                        validation_results[\"issues\"].append(issue)\n",
    "                        print(f\"      âŒ Hook {j+1} missing 'id'\")\n",
    "                    else:\n",
    "                        print(f\"      âœ… Hook: {hook['id']}\")\n",
    "            else:\n",
    "                issue = f\"Repository {i+1} 'hooks' field must be a list\"\n",
    "                validation_results[\"issues\"].append(issue)\n",
    "                print(f\"      âŒ Invalid hooks structure\")\n",
    "    \n",
    "    # 4. CI Configuration Validation (if present)\n",
    "    if 'ci' in precommit_config:\n",
    "        print(f\"\\nğŸš€ CI Configuration Validation:\")\n",
    "        ci_config = precommit_config['ci']\n",
    "        \n",
    "        recommended_ci_fields = [\n",
    "            'autofix_commit_msg', 'autofix_prs', 'autoupdate_branch',\n",
    "            'autoupdate_commit_msg', 'autoupdate_schedule'\n",
    "        ]\n",
    "        \n",
    "        for field in recommended_ci_fields:\n",
    "            if field in ci_config:\n",
    "                print(f\"   âœ… {field}: {ci_config[field]}\")\n",
    "            else:\n",
    "                print(f\"   â„¹ï¸  Optional CI field '{field}' not configured\")\n",
    "\n",
    "# 5. Best Practices Check\n",
    "print(f\"\\nğŸ¯ Best Practices Validation:\")\n",
    "\n",
    "best_practices = [\n",
    "    {\n",
    "        \"name\": \"Version pinning\",\n",
    "        \"check\": all('rev' in repo and repo['rev'] != 'HEAD' for repo in precommit_config.get('repos', [])),\n",
    "        \"description\": \"All repositories should have pinned versions (not HEAD)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Hook organization\",\n",
    "        \"check\": len(precommit_config.get('repos', [])) <= 10,\n",
    "        \"description\": \"Reasonable number of repositories (not overwhelming)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Security hooks present\",\n",
    "        \"check\": any('detect' in str(repo).lower() or 'secret' in str(repo).lower() \n",
    "                    for repo in precommit_config.get('repos', [])),\n",
    "        \"description\": \"Security-related hooks should be configured\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"File processing hooks\",\n",
    "        \"check\": any('nbstripout' in str(repo) or 'trailing-whitespace' in str(repo)\n",
    "                    for repo in precommit_config.get('repos', [])),\n",
    "        \"description\": \"File cleanup hooks should be present\"\n",
    "    }\n",
    "]\n",
    "\n",
    "passed_practices = 0\n",
    "for practice in best_practices:\n",
    "    status = \"âœ…\" if practice[\"check\"] else \"âš ï¸ \"\n",
    "    print(f\"   {status} {practice['name']}\")\n",
    "    print(f\"      {practice['description']}\")\n",
    "    if practice[\"check\"]:\n",
    "        passed_practices += 1\n",
    "\n",
    "validation_results[\"score\"] = (\n",
    "    (1 if validation_results[\"yaml_valid\"] else 0) + \n",
    "    (1 if len(validation_results[\"required_fields\"]) > 0 else 0) +\n",
    "    (1 if len(validation_results[\"issues\"]) == 0 else 0) +\n",
    "    (passed_practices / len(best_practices))\n",
    ") / 4 * 100\n",
    "\n",
    "print(f\"\\nğŸ“Š Validation Score: {validation_results['score']:.1f}/100\")\n",
    "\n",
    "if validation_results[\"score\"] >= 90:\n",
    "    print(\"ğŸŸ¢ Excellent: Configuration follows best practices\")\n",
    "elif validation_results[\"score\"] >= 75:\n",
    "    print(\"ğŸŸ¡ Good: Minor improvements possible\")\n",
    "elif validation_results[\"score\"] >= 50:\n",
    "    print(\"ğŸŸ  Fair: Several issues to address\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor: Significant problems with configuration\")\n",
    "\n",
    "if validation_results[\"issues\"]:\n",
    "    print(f\"\\nâš ï¸  Issues to address:\")\n",
    "    for issue in validation_results[\"issues\"]:\n",
    "        print(f\"   ğŸ”¸ {issue}\")\n",
    "\n",
    "validation_results[\"structure_valid\"] = len(validation_results[\"issues\"]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd96ac5f",
   "metadata": {},
   "source": [
    "## 7. Generate Configuration Report\n",
    "\n",
    "Creating a comprehensive report with recommendations, security assessment, and suggested improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfbbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive configuration report\n",
    "report = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"overall_score\": 0,\n",
    "    \"categories\": {},\n",
    "    \"recommendations\": [],\n",
    "    \"security_rating\": \"\",\n",
    "    \"summary\": \"\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š COMPREHENSIVE PRE-COMMIT CONFIGURATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ• Generated: {report['timestamp']}\")\n",
    "print(f\"ğŸ“ Configuration: .pre-commit-config.yaml\")\n",
    "\n",
    "# Calculate category scores\n",
    "category_scores = {\n",
    "    \"File Size Control\": {\n",
    "        \"score\": 85 if file_size_analysis[\"found\"] and file_size_analysis[\"max_size_kb\"] <= 1000 else \n",
    "                40 if file_size_analysis[\"found\"] else 0,\n",
    "        \"details\": f\"{'âœ…' if file_size_analysis['found'] else 'âŒ'} File size limit: {file_size_analysis.get('max_size_kb', 'Not set')} KB\"\n",
    "    },\n",
    "    \"Secret Detection\": {\n",
    "        \"score\": (secret_detection[\"coverage_score\"] / 4) * 100,\n",
    "        \"details\": f\"ğŸ” {secret_detection['coverage_score']}/4 security layers active\"\n",
    "    },\n",
    "    \"Notebook Management\": {\n",
    "        \"score\": 100 if notebook_config[\"nbstripout_found\"] and len(notebook_config[\"notebooks_with_outputs\"]) == 0 else\n",
    "                75 if notebook_config[\"nbstripout_found\"] else\n",
    "                50 if notebook_config[\"notebook_count\"] == 0 else 0,\n",
    "        \"details\": f\"ğŸ““ nbstripout: {'âœ…' if notebook_config['nbstripout_found'] else 'âŒ'}, Clean notebooks: {notebook_config['notebook_count'] - len(notebook_config['notebooks_with_outputs'])}/{notebook_config['notebook_count']}\"\n",
    "    },\n",
    "    \"Version Management\": {\n",
    "        \"score\": max(0, 100 - (version_analysis[\"outdated_count\"] * 20)),\n",
    "        \"details\": f\"ğŸ“¦ {len(version_analysis['repositories']) - version_analysis['outdated_count']}/{len(version_analysis['repositories'])} repos up-to-date\"\n",
    "    },\n",
    "    \"Configuration Quality\": {\n",
    "        \"score\": validation_results[\"score\"],\n",
    "        \"details\": f\"ğŸ”§ Structure: {'âœ…' if validation_results['structure_valid'] else 'âŒ'}, Issues: {len(validation_results['issues'])}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display category breakdown\n",
    "print(f\"\\nğŸ“‹ CATEGORY ANALYSIS:\")\n",
    "total_score = 0\n",
    "for category, data in category_scores.items():\n",
    "    score = data[\"score\"]\n",
    "    total_score += score\n",
    "    \n",
    "    if score >= 90:\n",
    "        grade = \"ğŸŸ¢ A\"\n",
    "    elif score >= 80:\n",
    "        grade = \"ğŸŸ¡ B\" \n",
    "    elif score >= 70:\n",
    "        grade = \"ğŸŸ  C\"\n",
    "    elif score >= 60:\n",
    "        grade = \"ğŸ”´ D\"\n",
    "    else:\n",
    "        grade = \"ğŸš¨ F\"\n",
    "    \n",
    "    print(f\"   {category:<20} {grade} ({score:3.0f}%) - {data['details']}\")\n",
    "    report[\"categories\"][category] = {\"score\": score, \"grade\": grade, \"details\": data[\"details\"]}\n",
    "\n",
    "report[\"overall_score\"] = total_score / len(category_scores)\n",
    "\n",
    "# Overall Assessment\n",
    "print(f\"\\nğŸ¯ OVERALL ASSESSMENT:\")\n",
    "print(f\"   ğŸ“Š Overall Score: {report['overall_score']:.1f}/100\")\n",
    "\n",
    "if report[\"overall_score\"] >= 90:\n",
    "    overall_grade = \"ğŸŸ¢ EXCELLENT\"\n",
    "    report[\"summary\"] = \"Outstanding configuration with comprehensive security and quality controls\"\n",
    "elif report[\"overall_score\"] >= 80:\n",
    "    overall_grade = \"ğŸŸ¡ GOOD\"\n",
    "    report[\"summary\"] = \"Strong configuration with minor areas for improvement\"\n",
    "elif report[\"overall_score\"] >= 70:\n",
    "    overall_grade = \"ğŸŸ  FAIR\"\n",
    "    report[\"summary\"] = \"Decent configuration but several important improvements needed\"\n",
    "elif report[\"overall_score\"] >= 60:\n",
    "    overall_grade = \"ğŸ”´ POOR\"\n",
    "    report[\"summary\"] = \"Significant configuration issues that should be addressed\"\n",
    "else:\n",
    "    overall_grade = \"ğŸš¨ CRITICAL\"\n",
    "    report[\"summary\"] = \"Major configuration problems requiring immediate attention\"\n",
    "\n",
    "print(f\"   ğŸ† Grade: {overall_grade}\")\n",
    "print(f\"   ğŸ“ {report['summary']}\")\n",
    "\n",
    "# Security Rating\n",
    "security_factors = [\n",
    "    secret_detection[\"coverage_score\"] >= 3,\n",
    "    file_size_analysis[\"found\"],\n",
    "    '.env' in str(precommit_config),  # Check if env files are being handled\n",
    "    validation_results[\"yaml_valid\"]\n",
    "]\n",
    "\n",
    "security_score = sum(security_factors) / len(security_factors) * 100\n",
    "\n",
    "if security_score >= 75:\n",
    "    report[\"security_rating\"] = \"ğŸ”’ HIGH\"\n",
    "elif security_score >= 50:\n",
    "    report[\"security_rating\"] = \"ğŸŸ¡ MEDIUM\"\n",
    "else:\n",
    "    report[\"security_rating\"] = \"ğŸš¨ LOW\"\n",
    "\n",
    "print(f\"\\nğŸ”’ SECURITY RATING: {report['security_rating']} ({security_score:.0f}%)\")\n",
    "\n",
    "# Generate Recommendations\n",
    "print(f\"\\nğŸ’¡ PRIORITY RECOMMENDATIONS:\")\n",
    "\n",
    "priority_recs = []\n",
    "\n",
    "if not file_size_analysis[\"found\"]:\n",
    "    priority_recs.append(\"ğŸ”´ HIGH: Add check-added-large-files hook immediately\")\n",
    "elif not file_size_analysis[\"max_size_kb\"]:\n",
    "    priority_recs.append(\"ğŸŸ¡ MEDIUM: Add explicit --maxkb argument to file size check\")\n",
    "\n",
    "if secret_detection[\"coverage_score\"] < 3:\n",
    "    priority_recs.append(\"ğŸ”´ HIGH: Enhance secret detection with more security layers\")\n",
    "\n",
    "if notebook_config[\"notebook_count\"] > 0 and not notebook_config[\"nbstripout_found\"]:\n",
    "    priority_recs.append(\"ğŸŸ  MEDIUM: Add nbstripout for Jupyter notebook cleaning\")\n",
    "\n",
    "if len(notebook_config[\"notebooks_with_outputs\"]) > 0:\n",
    "    priority_recs.append(\"ğŸŸ¡ LOW: Clean existing notebook outputs manually\")\n",
    "\n",
    "if version_analysis[\"outdated_count\"] > 2:\n",
    "    priority_recs.append(\"ğŸŸ¡ MEDIUM: Update outdated hook versions\")\n",
    "\n",
    "if not validation_results[\"structure_valid\"]:\n",
    "    priority_recs.append(\"ğŸ”´ HIGH: Fix YAML configuration issues\")\n",
    "\n",
    "if not priority_recs:\n",
    "    priority_recs.append(\"ğŸŸ¢ Configuration is excellent - no critical issues!\")\n",
    "\n",
    "for i, rec in enumerate(priority_recs[:5], 1):  # Top 5 recommendations\n",
    "    print(f\"   {i}. {rec}\")\n",
    "    report[\"recommendations\"].append(rec)\n",
    "\n",
    "# Implementation Steps\n",
    "print(f\"\\nğŸ› ï¸  NEXT STEPS:\")\n",
    "print(\"   1. Address HIGH priority recommendations first\")\n",
    "print(\"   2. Test configuration with: `pre-commit run --all-files`\")\n",
    "print(\"   3. Install hooks: `pre-commit install`\")\n",
    "print(\"   4. Monitor performance and adjust as needed\")\n",
    "print(\"   5. Schedule regular configuration reviews\")\n",
    "\n",
    "# Save report summary\n",
    "print(f\"\\nğŸ’¾ Report complete! Key metrics:\")\n",
    "print(f\"   â€¢ {len(precommit_config.get('repos', []))} repositories configured\")\n",
    "print(f\"   â€¢ {version_analysis['total_hooks']} total hooks\")\n",
    "print(f\"   â€¢ {secret_detection['coverage_score']}/4 security layers\")\n",
    "print(f\"   â€¢ {len(priority_recs)} recommendations\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Pre-commit configuration evaluation complete!\")\n",
    "print(\"   Use this analysis to improve your repository's quality and security.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
